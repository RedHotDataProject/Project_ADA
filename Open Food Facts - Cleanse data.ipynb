{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Our generated code\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from libs import exploring as explore\n",
    "from libs import visualising as visualize\n",
    "from libs import cleansing as cleanse\n",
    "\n",
    "PLOT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "open_food_facts_csv_file = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "\n",
    "# Load list of columns (external file) that are loaded into pyspark\n",
    "data = []\n",
    "with open(\"./data/cleanse/columns_to_import.txt\", \"r\") as json_data:\n",
    "    columns_to_import = json.load(json_data)\n",
    "    columns_to_import\n",
    "\n",
    "\n",
    "food_facts_pd = pd.read_csv(open_food_facts_csv_file,\n",
    "                            delimiter=\"\\t\",\n",
    "                            usecols=columns_to_import.keys(),\n",
    "                            dtype=columns_to_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_string = \"The dataset now comprises {} entries, of which we have {} features.\"\n",
    "data_summary_string.format(food_facts_pd.shape[0], food_facts_pd.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display number of NaN entries per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_entries = pd.DataFrame({'columns' : food_facts_pd.columns,\n",
    "                             'not nan_values' : [food_facts_pd[c].count() for c in food_facts_pd]\n",
    "                            })\n",
    "\n",
    "# Plot NaNs counts\n",
    "if PLOT:\n",
    "    null_entries.set_index('columns').plot(kind='barh', figsize=(10, 10))\n",
    "    plt.title(\"Not null values count in each column\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are many NaN entries in this data set. For our analysis, we can only use entries that have at least a product name, country tag, manufacturing and purchase place, and a created date tag. Unfortunately, we have to drop all columns, that lack those data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rows_inital = food_facts_pd.shape[0]\n",
    "\n",
    "# Drop entries with missing entries in of our main-features\n",
    "essential_columns = ['created_t', 'product_name', 'countries_en', 'categories_en', 'manufacturing_places', 'purchase_places']\n",
    "food_facts_pd = food_facts_pd.dropna(subset=essential_columns, )\n",
    "\n",
    "no_rows_reduced_nan = food_facts_pd.shape[0]\n",
    "\n",
    "# Also drop duplicated values (indentify based on index (barcode))\n",
    "food_facts_pd = food_facts_pd.drop_duplicates()\n",
    "\n",
    "no_rows_reduced_duplicates = food_facts_pd.shape[0]\n",
    "\n",
    "print(\"{} entries were dropped, of which {} were duplicates.\"\\\n",
    "      .format(no_rows_inital-no_rows_reduced_duplicates, no_rows_reduced_nan-no_rows_reduced_duplicates))\n",
    "print(data_summary_string.format(food_facts_pd.shape[0], food_facts_pd.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puhh, that was though. From now on, we are going to rescue the data and enrich wherever we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with emptry string\n",
    "food_facts_pd = food_facts_pd.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets look at the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we are not really keen of are the language indicators, so we are going to remove those abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_language_indicator(row_str):\n",
    "    tags = [tag if len(tag.split(':'))==1 else tag.split(':')[1] for tag in row_str.split(',')]\n",
    "    return \",\".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.categories_en = food_facts_pd.categories_en.apply(remove_language_indicator)\n",
    "food_facts_pd.main_category = food_facts_pd.main_category.apply(remove_language_indicator)\n",
    "food_facts_pd.countries_en = food_facts_pd.countries_en.apply(remove_language_indicator)\n",
    "food_facts_pd.labels_en = food_facts_pd.labels_en.apply(remove_language_indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next issue we are going to tackel are redudant columns. Especially here, these are similarly named columns ending with \"_en\", \"_tags\". We are handling this, by only importing columns that end with \"_en\" if we have the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitize tags\n",
    "Many parts of the data are categorizations based on tags. However, those tags are in a variety of languages and string formattings, so in order to use them we attempt to group tags that hint to the same property and map them to a common indicator. \n",
    "\n",
    "Every column of the data set requires special treatment, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd = food_facts_pd.dropna(subset=['product_name', 'countries_en', 'stores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :  \n",
    "- purchase_places and countries_en are the same though \"countries_en\" is more complete\n",
    "-  manufacturing_places and origins are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"./data/country_lookup.csv\")[['name', 'cca2', 'alias', 'Forced']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "copy_purchases_places = food_facts_pd[['purchase_places']].iloc[:2000, :]\n",
    "copy_purchases_places = copy_purchases_places.replace('', \"Unknown\", regex=True)\n",
    "copy_purchases_places['Filtered'] = copy_purchases_places.purchase_places.apply(lambda x: cleanse.country_name_filter(x, countries))\n",
    "copy_purchases_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    _,_ = visualize.plot_occurences_of_distinct_values(food_facts_pd, 'countries_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitze labels\n",
    "with open('./data/cleanse/taxonomies.json', 'r') as json_data:\n",
    "    labels_lookup = cleanse.to_lookup(json.load(json_data))\n",
    "food_facts_pd.labels_en = food_facts_pd.labels_en.apply(lambda x: \",\".join([labels_lookup[z] for z in x.split(',')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    _,_ = visualize.plot_occurences_of_distinct_values(food_facts_pd, 'labels_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store labels tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitize store labels\n",
    "with open('./data/cleanse/stores_lookup.json', 'r') as json_data:\n",
    "    stores_lookup = cleanse.to_lookup(json.load(json_data))\n",
    "food_facts_pd.stores = food_facts_pd.stores.fillna(\"\").apply(lambda x: \",\".join([stores_lookup[z] for z in x.split(',')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    _,_ = visualize.plot_occurences_of_distinct_values(food_facts_pd, 'stores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food category tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    _,_ = visualize.plot_occurences_of_distinct_values(food_facts_pd, 'categories_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon footprint dataset\n",
    "\n",
    "Because the food facts database lacks carbon footprint specifications, we got random samples of products from Eaternity database. The goal afterward is to match Food Facts Database and Carbon Footprint database. First, let's take a look at the Carbon Footprint database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "carbon_footprint_csv_file = \"./data/carbon_footprint.csv\"\n",
    "\n",
    "carbon_footprint_pd = pd.read_csv(carbon_footprint_csv_file,\n",
    "                            delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_footprint_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have {0} ecological features for {1} products.'\\\n",
    "      .format(carbon_footprint_pd.shape[1], \n",
    "              carbon_footprint_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_footprint_pd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The title of each product is in german. If we want to match with the previous dataset with some keywords for exemple, we should translate into the same language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install py-translate\n",
    "from translate import translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_row(row):\n",
    "    translation = translator('de', 'en', row)\n",
    "\n",
    "for i, row in enumerate(carbon_footprint_pd['Title']):\n",
    "    print(i,row)\n",
    "   #  carbon_footprint_pd.loc[i] = translate_row(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_footprint_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_footprint_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = carbon_footprint_pd['CO2-Value [gram CO2/serving]'].hist(bins=50)\n",
    "ax.set_xlabel('Carbon footprint')\n",
    "ax.set_ylabel('Occurencies')\n",
    "ax.set_title('Distribution of the carbon ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat price info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(\"./web_crawler/data/prices_carbon.csv\", dtype={'code':object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_reduced = prices[['product_name', 'price_per_100g', 'store_currency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# food_facts_pd.code = pd.to_numeric(food_facts_pd.code, errors='coerce').fillna(0).clip_upper(sys.maxsize).astype('int')\n",
    "# prices_reduced.code = prices_reduced.code.clip_upper(sys.maxsize).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we should here merge on the open food facts code, because it is guaranteed to be unique. However, it cannot be written to a text file, because pandas internally transforms it to an int, thereby removing preceding zeros, which we haven't found a workaround for yet.\n",
    "\n",
    "Also, it is consistent in the way that we query the prices by the product name, so we would obtain the same price for products of the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd = pd.merge(food_facts_pd, prices_reduced, on='product_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Product prices successfully merged: {}\".format(food_facts_pd.price_per_100g.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove negative entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = food_facts_pd.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns\n",
    "\n",
    "food_facts_pd[numeric_columns] = food_facts_pd[numeric_columns].where(food_facts_pd[numeric_columns] >= 0, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write clean data frame to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataframe that extracts all information required by the web crawler\n",
    "if 1==0: # skip cell\n",
    "    products = food_facts_pd[['code', 'product_name', 'stores', 'carbon-footprint_100g', 'nutrition-score-fr_100g']]\n",
    "    products = products[products['carbon-footprint_100g']!=\"\"]\n",
    "\n",
    "    products.to_pickle(\"./web_crawler/data/products_pd.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "clean_data_file_name = \"./data/openfoodfacts_clean.csv\"\n",
    "food_facts_pd.to_csv(clean_data_file_name, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADA",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

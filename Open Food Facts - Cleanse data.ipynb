{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Our generated code\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from libs import exploring as explore\n",
    "from libs import visualising as visualize\n",
    "from libs import cleansing as cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "open_food_facts_csv_file = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "\n",
    "# Load list of columns (external file) that are loaded into pyspark\n",
    "data = []\n",
    "with open(\"./data/cleanse/columns_to_import.txt\", \"r\") as json_data:\n",
    "    columns_to_import = json.load(json_data)\n",
    "    columns_to_import\n",
    "\n",
    "\n",
    "food_facts_pd = pd.read_csv(open_food_facts_csv_file,\n",
    "                            delimiter=\"\\t\",\n",
    "                            usecols=columns_to_import.keys(),\n",
    "                            dtype=columns_to_import,\n",
    "                            index_col='code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_string = \"The dataset now comprises {} entries, of which we have {} features.\"\n",
    "data_summary_string.format(food_facts_pd.shape[0], food_facts_pd.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display number of NaN entries per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_entries = pd.DataFrame({'columns' : food_facts_pd.columns,\n",
    "                             'not nan_values' : [food_facts_pd[c].count() for c in food_facts_pd]\n",
    "                            })\n",
    "\n",
    "# Plot NaNs counts\n",
    "null_entries.set_index('columns').plot(kind='barh', figsize=(10, 10))\n",
    "plt.title(\"Not null values count in each column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are many NaN entries in this data set. For our analysis, we can only use entries that have at least a product name, country tag, manufacturing and purchase place, and a created date tag. Unfortunately, we have to drop all columns, that lack those data. Because our interest lies in the carbon footprint, we have got a dataset of renadom samples from [Eaternity](http://www.eaternity.org/), providing a database for carrying out environmental calculations for menus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rows_inital = food_facts_pd.shape[0]\n",
    "\n",
    "# Drop entries with missing entries in of our main-features\n",
    "essential_columns = ['created_t', 'product_name', 'countries_en', 'categories_en', 'manufacturing_places', 'purchase_places']\n",
    "food_facts_pd = food_facts_pd.dropna(subset=essential_columns, )\n",
    "\n",
    "no_rows_reduced_nan = food_facts_pd.shape[0]\n",
    "\n",
    "# Also drop duplicated values (indentify based on index (barcode))\n",
    "food_facts_pd = food_facts_pd.drop_duplicates()\n",
    "\n",
    "no_rows_reduced_duplicates = food_facts_pd.shape[0]\n",
    "\n",
    "print(\"{} entries were dropped, of which {} were duplicates.\"\\\n",
    "      .format(no_rows_inital-no_rows_reduced_duplicates, no_rows_reduced_nan-no_rows_reduced_duplicates))\n",
    "print(data_summary_string.format(food_facts_pd.shape[0], food_facts_pd.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puhh, that was though. From now on, we are going to rescue the data and enrich wherever we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with emptry string\n",
    "food_facts_pd = food_facts_pd.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets look at the data types:\n",
    "food_facts_pd.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove some erros, some created_time have supermarket names instead of time format\n",
    "food_facts_pd = food_facts_pd[food_facts_pd['created_datetime'].map(len) == 20]\n",
    "food_facts_pd['created_datetime'] = pd.to_datetime(food_facts_pd['created_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we are not really keen of are the language indicators, so we are going to remove those abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_language_indicator(row_str):\n",
    "    tags = [tag if len(tag.split(':'))==1 else tag.split(':')[1] for tag in row_str.split(',')]\n",
    "    return \",\".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.categories_en = food_facts_pd.categories_en.apply(remove_language_indicator)\n",
    "food_facts_pd.main_category = food_facts_pd.main_category.apply(remove_language_indicator)\n",
    "food_facts_pd.countries_en = food_facts_pd.countries_en.apply(remove_language_indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next issue we are going to tackel are redudant columns. Especially here, these are similarly named columns ending with \"_en\", \"_tags\". We are handling this, by only importing columns that end with \"_en\" if we have the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitize tags\n",
    "Many parts of the data are categorizations based on tags. However, those tags are in a variety of languages and string formattings, so in order to use them we attempt to group tags that hint to the same property and map them to a common indicator. \n",
    "\n",
    "Every column of the data set requires special treatment, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd = food_facts_pd.dropna(subset=['product_name', 'countries_en', 'stores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :  \n",
    "- purchase_places and countries_en are the same though \"countries_en\" is more complete\n",
    "-  manufacturing_places and origins are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"./data/country_lookup.csv\")[['name', 'cca2', 'alias', 'Forced']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Filtered column with right names of countries\n",
    "copy_purchases_places = food_facts_pd[['purchase_places']]\n",
    "copy_purchases_places = copy_purchases_places.replace('', \"Unknown\", regex=True)\n",
    "copy_purchases_places['Filtered'] = copy_purchases_places.purchase_places.apply(lambda x: cleanse.country_name_filter(x, countries))\n",
    "copy_purchases_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_occurences_of_distinct_values(copy_purchases_places, 'Filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitze labels\n",
    "with open('./data/cleanse/taxonomies.json', 'r') as json_data:\n",
    "    labels_lookup = cleanse.to_lookup(json.load(json_data))\n",
    "food_facts_pd.labels = food_facts_pd.labels.apply(lambda x: [labels_lookup[z] for z in x.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_occurences_of_distinct_values(food_facts_pd, 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store labels tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitize store labels\n",
    "with open('./data/cleanse/stores_lookup.json', 'r') as json_data:\n",
    "    stores_lookup = cleanse.to_lookup(json.load(json_data))\n",
    "food_facts_pd.stores = food_facts_pd.stores.fillna(\"\").apply(lambda x: [stores_lookup[z] for z in x.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_occurences_of_distinct_values(food_facts_pd, 'stores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food category tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.stores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write clean data frame to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataframe that extracts all information required by the web crawler\n",
    "if 1==0: # skip cell\n",
    "    products = food_facts_pd\n",
    "\n",
    "    products.to_pickle(\"./web_crawler/products_pd.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "clean_data_file_name = \"./data/openfoodfacts_clean.csv\"\n",
    "food_facts_pd.to_csv(clean_data_file_name, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

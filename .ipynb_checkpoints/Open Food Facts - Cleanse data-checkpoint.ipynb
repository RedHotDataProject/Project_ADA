{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Our generated code\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from libs import exploring as explore\n",
    "from libs import visualising as visualize\n",
    "from libs import cleansing as cleanse\n",
    "\n",
    "PLOT = False\n",
    "RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "open_food_facts_csv_file = \"./data/en.openfoodfacts.org.products.csv\"\n",
    "\n",
    "# Load list of columns (external file) that are loaded into pyspark\n",
    "data = []\n",
    "with open(\"./data/cleanse/columns_to_import.txt\", \"r\") as json_data:\n",
    "    columns_to_import = json.load(json_data)\n",
    "    columns_to_import\n",
    "\n",
    "\n",
    "food_facts_pd = pd.read_csv(open_food_facts_csv_file,\n",
    "                            delimiter=\"\\t\",\n",
    "                            usecols=columns_to_import.keys(),\n",
    "                            dtype=columns_to_import)\n",
    "\n",
    "food_facts_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary_string = \"The dataset now comprises {} entries, of which we have {} features.\"\n",
    "data_summary_string.format(food_facts_pd.shape[0], food_facts_pd.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "We begin with taking a quick look on the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display number of non-NaN entries per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_entries = pd.DataFrame({'columns' : food_facts_pd.columns,\n",
    "                             'not nan_values' : [food_facts_pd[c].count() for c in food_facts_pd]\n",
    "                            })\n",
    "\n",
    "# Plot NaNs counts\n",
    "if PLOT:\n",
    "    null_entries.set_index('columns').plot(kind='barh', figsize=(10, 10))\n",
    "    plt.title(\"Not null values count in each column\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are many NaN entries in this data set. For our analysis, we can only use entries that have at least a product name, country tag, manufacturing and purchase place, store, and a created date tag. Unfortunately, we have to drop all columns, that lack these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rows_inital = food_facts_pd.shape[0]\n",
    "\n",
    "# Drop entries with missing entries in one of our main-features\n",
    "essential_columns = ['created_t', \n",
    "                     'product_name', \n",
    "                     'countries_en', \n",
    "                     'categories_en', \n",
    "                     'stores',\n",
    "                     'manufacturing_places', \n",
    "                     'purchase_places']\n",
    "\n",
    "food_facts_pd = food_facts_pd.dropna(subset=essential_columns, )\n",
    "\n",
    "no_rows_reduced_nan = food_facts_pd.shape[0]\n",
    "\n",
    "# Also drop duplicated values (indentify based on index (barcode))\n",
    "food_facts_pd = food_facts_pd.drop_duplicates()\n",
    "\n",
    "no_rows_reduced_duplicates = food_facts_pd.shape[0]\n",
    "\n",
    "print(\"{} entries were dropped, {} of those were duplicates.\"\\\n",
    "      .format(no_rows_inital-no_rows_reduced_duplicates, \n",
    "              no_rows_reduced_nan-no_rows_reduced_duplicates)\n",
    "     )\n",
    "print(data_summary_string.format(food_facts_pd.shape[0], \n",
    "                                 food_facts_pd.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puhh, that was though. From now on, we are going to rescue the data and enrich wherever we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with emptry string\n",
    "food_facts_pd = food_facts_pd.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets look at the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we are not really keen of are the language indicators, so we are going to remove those abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_language_indicator(row_str):\n",
    "    tags = [tag if len(tag.split(':'))==1 else tag.split(':')[1] for tag in row_str.split(',')]\n",
    "    return \",\".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.categories_en = food_facts_pd.categories_en.apply(remove_language_indicator)\n",
    "food_facts_pd.main_category = food_facts_pd.main_category.apply(remove_language_indicator)\n",
    "food_facts_pd.countries_en = food_facts_pd.countries_en.apply(remove_language_indicator)\n",
    "food_facts_pd.labels_en = food_facts_pd.labels_en.apply(remove_language_indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitize tags\n",
    "Many parts of the data are categorizations based on tags. However, those tags are in a variety of languages and string formattings, so in order to use them we attempt to group tags that hint to the same property and map them to a common indicator. \n",
    "\n",
    "Every column of the data set requires special treatment, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :  \n",
    "- purchase_places and countries_en are the same though \"countries_en\" has more entries\n",
    "- manufacturing_places and origins are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"_Countries_\" is a csv file modified in the \"_Country__names.ipynb_\" file from the source (available at https://mledoze.github.io/countries/). We need to harmonise country names (and push them to English since many entries use French and German). The columns requiring our attentions are the following:\n",
    "- origins\n",
    "- manufacturing_places\n",
    "- countries_en\n",
    "\n",
    "Note that each have a respective redundant column : origins_tags, manufacturing_places_tags and purchase_places. We are going to filter these by a function in our _cleansing.py_ library to lead to the following respective columns:\n",
    "- origins_cleaned\n",
    "- manufacturing_place_cleaned\n",
    "- purchase_places_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd['origins_cleaned'] = food_facts_pd.origins\n",
    "food_facts_pd['manufacturing_place_cleaned'] = food_facts_pd.manufacturing_places\n",
    "food_facts_pd['purchase_places_cleaned'] = food_facts_pd.countries_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analyse file\n",
    "countries = pd.read_csv(\"./data/country_lookup.csv\")[['name', 'cca2', 'alias', 'Forced']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a test to see how complete the harmonisation is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's attack the Open Food Fact database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following commands should not be run except if the analysis has to be performed again. \n",
    "#Access the result in ./data/food_facts_pd_countries_names.csv (will be saved to that)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( RUN ):\n",
    "    food_facts_pd.origins_cleaned = food_facts_pd.origins_cleaned\\\n",
    "        .apply(lambda x: cleanse.country_name_filter(x, countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( RUN ):\n",
    "    food_facts_pd.manufacturing_place_cleaned = food_facts_pd.manufacturing_place_cleaned\\\n",
    "        .apply(lambda x: cleanse.country_name_filter(x, countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( RUN ):\n",
    "    food_facts_pd.purchase_places_cleaned = food_facts_pd.purchase_places_cleaned\\\n",
    "        .apply(lambda x: cleanse.country_name_filter(x, countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save these columns so that we don't have to run them again. \n",
    "\n",
    "#Do not run this command if you have not processed the whole dataset !\n",
    "if( RUN ):\n",
    "    food_facts_pd_countries_names = food_facts_pd[['origins_cleaned', \n",
    "                                                   'manufacturing_place_cleaned', \n",
    "                                                   'purchase_places_cleaned']\n",
    "                                                 ]\n",
    "    food_facts_pd_countries_names.to_csv(\"./data/food_facts_pd_countries_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitze labels\n",
    "with open('./data/cleanse/taxonomies.json', 'r') as json_data:\n",
    "    labels_lookup = cleanse.to_lookup(json.load(json_data))\n",
    "food_facts_pd.labels_en = food_facts_pd.labels_en.\\\n",
    "    apply(lambda x: [labels_lookup[z] for z in x.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    _,_ = visualize.plot_occurences_of_distinct_values(food_facts_pd, 'labels_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store labels tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitize store labels\n",
    "with open('./data/cleanse/stores_lookup.json', 'r') as json_data:\n",
    "    stores_lookup = cleanse.to_lookup(json.load(json_data))\n",
    "food_facts_pd.stores = food_facts_pd.stores.fillna(\"\")\\\n",
    "    .apply(lambda x: [stores_lookup[z] for z in x.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    _,_ = visualize.plot_occurences_of_distinct_values(food_facts_pd, 'stores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food category tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group categories by user-defined themes\n",
    "with open('./data/cleanse/categories_en_lookup.json', 'r') as json_data:\n",
    "    categories_en_lookup = cleanse.to_lookup(json.load(json_data))\n",
    "food_facts_pd.main_category = food_facts_pd.categories_en.\\\n",
    "    apply(cleanse.group_categories, args=[categories_en_lookup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    _,_ = visualize.plot_occurences_of_distinct_values_from_strings(food_facts_pd, 'main_category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon footprint dataset\n",
    "\n",
    "Because the food facts database lacks carbon footprint specifications, we got random samples of products from Eaternity database. Unfortunately, we were not allowed access to the API before purchasing a 2000 CHF license. However, this weekend, we were given a dataset of carbon footprints for specific products. To use it for our analysis, we have to categorize this products, as they do not match with the food items in the food facts database.\n",
    "\n",
    "Let's still take a quick look at the Carbon Footprint database, that we have obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "carbon_footprint_csv_file = \"./data/carbon_footprint.csv\"\n",
    "\n",
    "carbon_footprint_pd = pd.read_csv(carbon_footprint_csv_file, delimiter=\",\")\n",
    "\n",
    "#Import data with categories \n",
    "# Import data\n",
    "carbon_footprint_categories_csv_file = \"./data/carbon_footprint_categories.csv\"\n",
    "\n",
    "carbon_footprint_categories_pd = pd.read_csv(carbon_footprint_categories_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have {0} ecological features for {1} products.'\\\n",
    "      .format(carbon_footprint_pd.shape[1], \n",
    "              carbon_footprint_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the carbon footprint of each product. Because our sample is small (around 700 products) and doesn't really match with the Food Facts Database, we will take care of the categories. Thus, we will extract the categories from [Codecheck website](https://www.codecheck.info/) (Webscraper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = carbon_footprint_pd['CO2-Value [gram CO2/serving]'].hist(bins=50)\n",
    "ax.set_xlabel('Carbon footprint')\n",
    "ax.set_ylabel('Occurencies')\n",
    "ax.set_title('Distribution of the carbon ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translation of category column in two times because limitation in translations\n",
    "translated_column = cleanse.translate_columns(carbon_footprint_categories_pd['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = translated_column[:453]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_column_2 = cleanse.translate_columns(carbon_footprint_categories_pd['category'].iloc[453:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_column_2\n",
    "translation2 = TR + translated_column_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_footprint_categories_pd['category_en'] = translation2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat price info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also searched online shops for price information about some of the products, that we are going to merge in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(\"./web_crawler/data/prices_carbon.csv\", dtype={'code':object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_reduced = prices[['product_name', 'price_per_100g', 'store_currency']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the OpenFoodFacts code is not necessarily a global barcode for the product, we had to match the products from the online stores and the entries of the database by teh product name. To be consistent with that method, we merge the prices by the product name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd = pd.merge(food_facts_pd, prices_reduced, on='product_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Product prices successfully merged: {}\".format(food_facts_pd.price_per_100g.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove negative entries\n",
    "We have few numerical features, which are the calories (energy), carbon-footprint, and price per 100g of the product. These values are due to physical or economic laws non-negative, which we are going to ensure in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = food_facts_pd.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns\n",
    "\n",
    "food_facts_pd[numeric_columns] = food_facts_pd[numeric_columns].where(food_facts_pd[numeric_columns] >= 0, np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write clean data frame to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataframe that extracts all information required by the web crawler\n",
    "if 1==0: # skip cell\n",
    "    products = food_facts_pd[['code', \n",
    "                              'product_name', \n",
    "                              'stores', \n",
    "                              'carbon-footprint_100g', \n",
    "                              'nutrition-score-fr_100g']\n",
    "                            ]\n",
    "    products = products[products['carbon-footprint_100g']!=\"\"]\n",
    "\n",
    "    products.to_pickle(\"./web_crawler/data/products_pd.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : country names harmonised are available in ./data/food_facts_pd_countries_names.csv (note the additional code to go back to a list of strings). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply country name filter\n",
    "\n",
    "countries_names= pd.read_csv(\"./data/food_facts_pd_countries_names.csv\")\n",
    "countries_names.origins_cleaned = \\\n",
    "                        countries_names.origins_cleaned.apply(lambda l: cleanse.read(l))\n",
    "\n",
    "countries_names.manufacturing_place_cleaned = \\\n",
    "                        countries_names.manufacturing_place_cleaned.apply(lambda l: cleanse.read(l))\n",
    "\n",
    "countries_names.purchase_places_cleaned = \\\n",
    "                        countries_names.purchase_places_cleaned.apply(lambda l: cleanse.read(l))\n",
    "\n",
    "food_facts_pd = food_facts_pd.drop(['origins', \n",
    "                                    'manufacturing_places', \n",
    "                                    'countries_en',\n",
    "                                   'origins_tags', \n",
    "                                    'manufacturing_places_tags',\n",
    "                                    'purchase_places'], \\\n",
    "                                    axis=1)\n",
    "\n",
    "food_facts_pd['origins_cleaned'] = countries_names.origins_cleaned\n",
    "food_facts_pd['manufacturing_place_cleaned'] = countries_names.manufacturing_place_cleaned\n",
    "food_facts_pd['purchase_places_cleaned'] = countries_names.purchase_places_cleaned\n",
    "food_facts_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.origins_cleaned.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.origins_cleaned= food_facts_pd.origins_cleaned.fillna(\"['Unknown']\")\n",
    "food_facts_pd.manufacturing_place_cleaned= food_facts_pd.manufacturing_place_cleaned.fillna(\"['Unknown']\")\n",
    "food_facts_pd.purchase_places_cleaned= food_facts_pd.purchase_places_cleaned.fillna(\"['Unknown']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_pd.origins_cleaned.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "clean_data_file_name = \"./data/openfoodfacts_clean.csv\"\n",
    "food_facts_pd.to_csv(clean_data_file_name, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_carbon_data_file_name = \"./data/carbon_footprint_categories.csv\"\n",
    "carbon_footprint_categories_pd.to_csv(processed_carbon_data_file_name, sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
